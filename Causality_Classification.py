# -*- coding: utf-8 -*-
"""Causality_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17EVV-uoHxXBBnm6MRQgXq8_P8E-hnL1X
"""

#import packages
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfTransformer
import io
import nltk, re, pprint
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import string
import psycopg2 as ps
from google.colab import files
nltk.download('punkt')
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer() 
nltk.download('stopwords')
nltk.download('wordnet')
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer = "word", binary = True, ngram_range=(3,3))
import spacy
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from keras.models import Sequential
from keras.layers import Dense, Conv1D, Flatten
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
from keras.utils import to_categorical
from nltk import word_tokenize
from collections import defaultdict
from collections import Counter
from keras.utils import to_categorical

#read in data
uploaded = files.upload()
dataset = pd.read_excel(io.BytesIO(uploaded['training_data.xlsx']))

#convert into lists
df = pd.DataFrame({'label':dataset.causal_relationship, 'text':dataset.sentence, 
                   'node1':dataset.node_1, 'node2':dataset.node_2})
df = df.dropna()
description_list = df['text'].tolist()
varietal_list = df['label'].tolist()
node1_list = df['node1'].tolist()
node2_list = df['node2'].tolist()

#normalization: lower case
i = 0
for n in node2_list: 
  node2_list[i] = n.lower()
  i = i + 1

j = 0
for n in node1_list:
  node1_list[j] = n.lower()
  j = j + 1

k = 0
for n in description_list: 
  description_list[k] = n.lower()
  k = k + 1

#entity replacement: node1 and node2
k = 0
for line in description_list: 
  str = description_list[k]
  description_list[k] = str.replace(node1_list[k], "node1")
  str = description_list[k]
  description_list[k] = str.replace(node2_list[k], "node2")
  k = k + 1

#tokenization
description_list = [nltk.word_tokenize(sent) for sent in description_list]

#remove stop words
punctuation = list(string.punctuation)
stop = stopwords.words('english') + punctuation + ['']

for i in range(len(description_list)): 
    description_list[i] = [item for item in description_list[i].copy() if item not in stop ]

print(description_list[0])

#stem
for i in range(len(description_list)): 
  description_list[i] = [lemmatizer.lemmatize(w) for w in description_list[i].copy()]

#trimming
full_list = []
for i in range(len(description_list)):
  list_hold = description_list[i]
  index1 = list_hold.index("node1")
  index2 = list_hold.index("node2")
  result1 = ''
  result2 = ''
  result3 = ''
  hold = []
  for j in range(index1):
    result1 += list_hold[j]
    if(j < index1 - 1):
      result1 += " "
  for j in range(index1 + 1, index2):
    result2 += list_hold[j]
    if(j < index2 - 1):
      result2 += " "
  
  if(index2 + 1 < len(list_hold)):
    for j in range(index2 + 1, len(list_hold)):
      result3 += list_hold[j]
  
  if result1 != '':
    hold.append(result1)
  
  hold.append(list_hold[index1])

  if result2 != '':
    hold.append(result2)

  hold.append(list_hold[index2])

  if result3 != '':
    hold.append(result3)

  full_list.append(hold)

#Bag of Words
full_list_strings = []
for i in range(len(full_list)):
  full_list_strings.append(" ".join(full_list[i]))
data_features = vectorizer.fit_transform(full_list_strings).toarray()

#Embedding
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import os
import gensim
def read_corpus(d_list):
  for i in range(len(d_list)):
    str1 = ""  
    s = d_list[i]
    for ele in s:  
      str1 += ele
      str1 += " "  
    tokens = gensim.utils.simple_preprocess(str1)
    yield gensim.models.doc2vec.TaggedDocument(tokens, [i])

train_corpus = list(read_corpus(description_list))
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)
model.build_vocab(train_corpus)

#train model
model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)

#Doc2Vec embeddings
embeddings = []
for i in range(len(full_list)):
  vector = model.infer_vector(full_list[i])
  vector = np.array(vector)
  embeddings.append(vector)

#bag-of-words embedding
embeddings_bow = []
labels = df.label
data_bow = np.zeros((len(full_list),1306))
for i in range(len(full_list)):
  row = data_features[i]
  data_bow[i,] = row

labels = df.label.tolist()
data = np.zeros((len(full_list),50))

for i in range(len(full_list)):
  row = embeddings[i]
  data[i,] = row

varietal_list_o = df['label'].tolist()
varietal_list_o = [int(i) for i in varietal_list_o]
varietal_list = to_categorical(varietal_list_o)

#train models with Doc2Vec features
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.25)

#logitstic regression using Doc2Vec features
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
clf = LogisticRegression(C=1e5)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))

#svm using Doc2Vec features
clf = SVC(kernel='linear').fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))

#train models with BOW features
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(data_bow, labels, test_size=0.25)

#logistic regression bow features
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
clf = LogisticRegression(C=1e5)
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))

#naive bayes bow features
clf = MultinomialNB().fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))

#svm bow features
clf = SVC(kernel='linear').fit(x_train, y_train)
y_pred = clf.predict(x_test)
print(classification_report(y_test, y_pred))
